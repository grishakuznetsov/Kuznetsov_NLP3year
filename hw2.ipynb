{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JWId0QdGEwi",
        "outputId": "4eaa7ec4-f7d0-464a-9012-96aa1228e420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install natasha\n",
        "!pip install spacy\n",
        "!pip install flair"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: natasha in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: slovnet>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.4.0)\n",
            "Requirement already satisfied: yargy>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.14.0)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.9.1)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from navec>=0.9.0->natasha) (1.18.5)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2->natasha) (0.6.2)\n",
            "Requirement already satisfied: intervaltree>=3 in /usr/local/lib/python3.6/dist-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.2.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n",
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.6.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: bpemb>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.7)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.6.2)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.10)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.41.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2019.12.20)\n",
            "Requirement already satisfied: pytest>=5.3.2 in /usr/local/lib/python3.6/dist-packages (from flair) (6.1.1)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.10)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (2.8.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.91)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.7.0)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.6/dist-packages (from flair) (5.8)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from flair) (4.2.6)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (from flair) (1.0.8)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.6/dist-packages (from flair) (0.4.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0+cu101)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->flair) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown->flair) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (2.2.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.11.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: overrides==3.0.0 in /usr/local/lib/python3.6/dist-packages (from konoha<5.0.0,>=4.0.0->flair) (3.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.2.0)\n",
            "Requirement already satisfied: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.13.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.12; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (2.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (20.4)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (0.10.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.1.1)\n",
            "Requirement already satisfied: py>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from pytest>=5.3.2->flair) (1.9.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair) (0.16.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.8.1rc2)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers>=3.0.0->flair) (0.0.43)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->flair) (3.0.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest>=5.3.2->flair) (3.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.0.0->flair) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrYVfIg4RkbL",
        "outputId": "1ca54e92-1e57-4c18-d9a8-be28ff362e1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "from natasha import (\n",
        "    Segmenter,\n",
        "    MorphVocab,\n",
        "    \n",
        "    NewsEmbedding,\n",
        "    NewsMorphTagger,\n",
        "    NewsSyntaxParser,\n",
        "    NewsNERTagger,\n",
        "    \n",
        "    PER,\n",
        "    NamesExtractor,\n",
        "\n",
        "    Doc\n",
        ")\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "syntax_parser = NewsSyntaxParser(emb)\n",
        "ner_tagger = NewsNERTagger(emb)\n",
        "\n",
        "names_extractor = NamesExtractor(morph_vocab)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "morph = MorphAnalyzer()\n",
        "from pymystem3 import Mystem\n",
        "my = Mystem()\n",
        "import spacy\n",
        "import flair\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "tagger = SequenceTagger.load('pos')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from string import punctuation\n",
        "from sklearn.metrics import accuracy_score\n",
        "import re"
      ],
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-18 19:52:54,473 loading file /root/.flair/models/en-pos-ontonotes-v0.5.pt\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4IknrD4tg50",
        "outputId": "a241b018-9301-46b6-cbd2-1b7492042b71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /root/.local/bin/mystem"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-18 19:52:54--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.244, 5.45.205.245, 5.45.205.241, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.244|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://cache-mskm902.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz [following]\n",
            "--2020-10-18 19:52:55--  http://cache-mskm902.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving cache-mskm902.cdn.yandex.net (cache-mskm902.cdn.yandex.net)... 5.45.220.12, 2a02:6b8:0:2002::13\n",
            "Connecting to cache-mskm902.cdn.yandex.net (cache-mskm902.cdn.yandex.net)|5.45.220.12|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16457938 (16M) [application/octet-stream]\n",
            "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz.2’\n",
            "\n",
            "mystem-3.0-linux3.1 100%[===================>]  15.70M  8.86MB/s    in 1.8s    \n",
            "\n",
            "2020-10-18 19:52:57 (8.86 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz.2’ saved [16457938/16457938]\n",
            "\n",
            "mystem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W2OO6WndBgj"
      },
      "source": [
        "def clean(text):\n",
        "  for i in punctuation:\n",
        "    text = text.replace(i, '')\n",
        "  return text.lower()"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcEvRif1SZPP"
      },
      "source": [
        "with open('ru_text.txt', 'r', encoding='utf-8') as f:\n",
        "  for_syntax = f.read()\n",
        "with open('ru_text.txt', 'r', encoding='utf-8') as f:\n",
        "  ru_text = clean(f.read()).split('\\n')\n",
        "with open('eng_text.txt', 'r', encoding='utf-8') as f:\n",
        "  eng_text = clean(f.read()).split('\\n')"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JvLArTeVR-k",
        "outputId": "66bb8ddc-dc45-4467-f196-494e1a400147",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "ru_text"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['сложно самому написать текст для парсера ',\n",
              " 'это словно забираться на тяньшань с закрытыми глазами ',\n",
              " 'справится джеки чан чак норрис или михаил пореченков ',\n",
              " 'водить машину с мкпп сложнее чем с акпп',\n",
              " 'мы не ели много мороженного но смотрели на ели ',\n",
              " 'мой кит очень усталый говорит что ему плохо',\n",
              " 'петя сделал качественный коммит на гитхабе читая книгу толстого на покетбуке',\n",
              " 'согласно данным сми на митинге побывало около четырех тыс человек',\n",
              " 'крокодил крокожу и буду крокодить',\n",
              " 'навстречу мне двигалось сто вась крича и прыгая',\n",
              " 'вдруг явился посол и начал танцевать яростно',\n",
              " 'на паре сидели пятьдесят студентов и они обсуждали множество непонятных и неоднозначных вопросов',\n",
              " 'это будет последнее предложение это не странно',\n",
              " 'петя взял стакан из стекла а по нему стекла вода',\n",
              " 'я положил пирожки в печь и начал их печь',\n",
              " 'задание не простое а очень сложное']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zVXQjEFVSjx"
      },
      "source": [
        "Сложности текста: омонимы ели мороженное - смотрели на ели, стакан из стекла - стекла вода, аббривеатуры МКПП АКПП, выдуманные слова крокожу, буду крокодить, книга Толстого - Толстого может быть сущ или прил"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0g7UT--VsqG",
        "outputId": "85997ab9-62e8-4ada-f7c2-e66d9a5909ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "eng_text"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i am very hungry i want to eat pizza',\n",
              " 'this is a text for the parsing',\n",
              " 'mr smith took the drink and they drink it',\n",
              " 'she loves to swim and she do it very well',\n",
              " 'mrs smith loves to smith because she is a smith',\n",
              " 'however bob has eaten one green orange',\n",
              " 'he asked me not very calmly about my new laptop',\n",
              " 'the final match of the uefa football league is today',\n",
              " 'mary thinked about orange juice',\n",
              " 'all the people are in danger',\n",
              " 'mr brown throwed the ring in the river and she had to ring the bell']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHgbnqIyVuxQ"
      },
      "source": [
        "Сложности: orange как апельсин и как оранжевый, аббривиатура UEFA, to ring и ring кольцо, три слова smith - два глагола и одно существительное"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUYNL46dSdZG"
      },
      "source": [
        "ru_words = []\n",
        "for i in ru_text:\n",
        "  for j in clean(i).split():\n",
        "    ru_words.append(j)"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG2b1PRtXsQ5"
      },
      "source": [
        "eng_words = []\n",
        "for i in eng_text:\n",
        "  for j in clean(i).split():\n",
        "    eng_words.append(j)"
      ],
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlwCQYIBYmgi"
      },
      "source": [
        "ru_tags = ['ADV PRON VERB NOUN PREP NOUN',\n",
        "           'PRON CONJ VERB PREP NOUN PREP ADJ NOUN',\n",
        "           'VERB NOUN NOUN NOUN NOUN PREP NOUN NOUN',\n",
        "           'VERB NOUN PREP NOUN ADJ CONJ PREP NOUN',\n",
        "           'PRON PART VERB ADV NOUN CONJ VERB PREP NOUN',\n",
        "           'PRON NOUN ADV ADJ VERB CONJ PRON ADV',\n",
        "           'NOUN VERB ADJ NOUN PREP NOUN VERB NOUN NOUN PREP NOUN',\n",
        "           'PREP NOUN NOUN PREP NOUN VERB ADV NUM NUM NOUN',\n",
        "           'NOUN VERB CONJ VERB VERB',\n",
        "           'ADV PRON VERB NUM NOUN VERB CONJ VERB',\n",
        "           'ADV VERB NOUN CONJ VERB VERB ADV',\n",
        "           'PREP NOUN VERB NUM NOUN CONJ PRON VERB NOUN ADJ CONJ ADJ NOUN',\n",
        "           'PRON VERB ADJ NOUN PRON PART ADV',\n",
        "           'NOUN VERB NOUN PREP NOUN CONJ PREP PRON VERB NOUN',\n",
        "           'PRON VERB NOUN PREP NOUN CONJ VERB PRON VERB',\n",
        "           'NOUN PART ADJ CONJ ADV ADJ'\n",
        "           ]"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKB23mf6QUqx"
      },
      "source": [
        "eng_tags = ['PRON VERB ADV ADJ PRON VERB PART VERB NOUN',\n",
        "            'DET VERB DET NOUN PREP DET NOUN',\n",
        "            'NOUN NOUN VERB DET NOUN CONJ PRON VERB PRON',\n",
        "            'PRON VERB PART VERB CONJ PRON VERB PRON ADV ADJ',\n",
        "            'NOUN NOUN VERB PART VERB CONJ NOUN VERB DET NOUN',\n",
        "            'CONJ NOUN VERB VERB NUM ADJ NOUN',\n",
        "            'PRON VERB PRON PART ADV ADJ PREP PRON ADJ NOUN',\n",
        "            'DET ADJ NOUN PREP DET NOUN ADJ NOUN VERB NOUN',\n",
        "            'PRON VERB PREP ADJ NOUN',\n",
        "            'PRON DET NOUN VERB PREP NOUN',\n",
        "            'NOUN NOUN VERB DET NOUN PREP DET NOUN CONJ PRON VERB VERB VERB DET NOUN'\n",
        "            ]"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9IEMIBkl6bG"
      },
      "source": [
        "ru_tags_all = []\n",
        "for i in ru_tags:\n",
        "  for j in i.split():\n",
        "    ru_tags_all.append(j)\n",
        "eng_tags_all = []\n",
        "for i in eng_tags:\n",
        "  for j in i.split():\n",
        "    eng_tags_all.append(j)"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvGnhKAef91w"
      },
      "source": [
        "def pym_normalized(text):\n",
        "  text = re.sub(r'ADJ\\w', 'ADJ', text)\n",
        "  text = text.replace('COMP', 'ADJ')\n",
        "  text = text.replace('NUMR', 'NUM')\n",
        "  text = text.replace('ADVB', 'ADV')\n",
        "  text = text.replace('INFN', 'VERB')\n",
        "  text = text.replace('GRND', 'VERB')\n",
        "  text = text.replace('NPRO', 'PRON')\n",
        "  text = text.replace('PRCL', 'PART')\n",
        "  if text is None:\n",
        "    text = ''\n",
        "  return text"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtM06zGJcSG-"
      },
      "source": [
        "pym_tags = []\n",
        "for i in ru_words:\n",
        "  pym_tags.append(pym_normalized(morph.parse(i)[0].tag.POS))"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C56tbeH5xurS"
      },
      "source": [
        "def mys_normalized(text):\n",
        "  text = re.sub('\\w*PRO\\w*', 'PRON', text)\n",
        "  text = re.sub('^A$', 'ADJ', text)\n",
        "  text = re.sub('^V$', 'VERB', text)\n",
        "  text = re.sub('^S$', 'NOUN', text)\n",
        "  text = re.sub('^PR$', 'PREP', text)\n",
        "  return text"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwyPtMs2pqh-",
        "outputId": "286b9a1f-90d8-42d1-eeb0-d1a25c484c60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "mys_tags = []\n",
        "for i in ru_words:\n",
        "  if my.analyze(i)[0]['analysis']:\n",
        "    mys_tags.append(mys_normalized(re.search(\n",
        "        r'[A-Z]+',my.analyze(i)[0]['analysis'][0]['gr']).group()))\n",
        "  else:\n",
        "    print(i)\n",
        "    mys_tags.append('')"
      ],
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "мкпп\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSruCTc5E7jI"
      },
      "source": [
        "def nts_normalized(tag):\n",
        "  tag = re.sub('PROPN', 'NOUN', tag)\n",
        "  tag = re.sub('ADP', 'PREP', tag)\n",
        "  tag = re.sub('\\wCONJ', 'CONJ', tag)\n",
        "  tag = re.sub('AUX', 'VERB', tag)\n",
        "  return tag"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FclKdOgj9EgB"
      },
      "source": [
        "nts_tags = []\n",
        "text = ''\n",
        "for i in ru_text:\n",
        "  text += clean(i) + ' '\n",
        "doc = Doc(text)\n",
        "\n",
        "doc.segment(segmenter)\n",
        "doc.tag_morph(morph_tagger)\n",
        "for i in doc.tokens:\n",
        "  nts_tags.append((nts_normalized(i.pos)))"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iis3j0dgWIvv"
      },
      "source": [
        "# Точность русских теггеров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B66Hq7mOM3-P",
        "outputId": "861faca9-264f-4b93-b448-f21fe8a12fe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print('Pymorphy: ', accuracy_score(pym_tags, ru_tags_all))\n",
        "print('Mystem: ', accuracy_score(mys_tags, ru_tags_all))\n",
        "print('Natasha: ', accuracy_score(nts_tags, ru_tags_all))"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pymorphy:  0.8872180451127819\n",
            "Mystem:  0.849624060150376\n",
            "Natasha:  0.9022556390977443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8AyWWhcaNQD"
      },
      "source": [
        "def spc_normalized(tag):\n",
        "  tag = tag.replace('ADP', 'PREP')\n",
        "  tag = tag.replace('PROPN', 'NOUN')\n",
        "  tag = tag.replace('AUX', 'VERB')\n",
        "  tag = re.sub(r'\\wCONJ', 'CONJ', tag)\n",
        "  return tag"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3T0wFCjKQkN"
      },
      "source": [
        "spc_tags = []\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "eng_str = ''\n",
        "for i in eng_text:\n",
        "  eng_str += i + ' '\n",
        "doc = nlp(eng_str)\n",
        "for token in doc:\n",
        "  spc_tags.append(spc_normalized(token.pos_))"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaA0y-Koa04-"
      },
      "source": [
        "flair_tags = []\n",
        "sent = Sentence(eng_str)\n",
        "tagger.predict(sent)\n",
        "tags_str = sent.to_tagged_string()"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH68Z3X0clsi"
      },
      "source": [
        "def flr_normalized(tag):\n",
        "  tag = re.sub(r'PRP.?', 'PRON', tag)\n",
        "  tag = re.sub(r'VB\\w?', 'VERB', tag)\n",
        "  tag = re.sub(r'^RB$', 'ADV', tag)\n",
        "  tag = tag.replace('TO', 'PART')\n",
        "  tag = re.sub(r'NN\\w?', 'NOUN', tag)\n",
        "  tag = tag.replace('DT', 'DET')\n",
        "  tag = tag.replace('JJ', 'ADJ')\n",
        "  tag = tag.replace('CC', 'CONJ')\n",
        "  tag = tag.replace('WRB', 'CONJ')\n",
        "  tag = tag.replace('IN', 'PREP')\n",
        "  tag = tag.replace('UH', 'NOUN')\n",
        "  tag = tag.replace('PDT', 'PRON')\n",
        "  tag = tag.replace('CD', 'NUM')\n",
        "  return tag"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJBvN8kPcDxA"
      },
      "source": [
        "flr_tags = [flr_normalized(x) for x in re.findall(r'<(\\w*.?)>', tags_str)]"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2shQTt-thAgr"
      },
      "source": [
        "nltk_tags = []\n",
        "text = word_tokenize(eng_str)\n",
        "tags = nltk.pos_tag(text)\n",
        "for i in tags:\n",
        "  nltk_tags.append(flr_normalized(i[1]))"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKUyDrsCWOQ5"
      },
      "source": [
        "# Точность английских теггеров"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3MteDenWR2_",
        "outputId": "7b7569be-f439-425f-ee8a-539f8d115bc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print('Spacy: ', accuracy_score(spc_tags, eng_tags_all))\n",
        "print('Flair: ', accuracy_score(flr_tags, eng_tags_all))\n",
        "print('NLTK: ', accuracy_score(nltk_tags, eng_tags_all))"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy:  0.8673469387755102\n",
            "Flair:  0.8775510204081632\n",
            "NLTK:  0.8163265306122449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQ_nHZ42kCBO"
      },
      "source": [
        "ru_str = ''\n",
        "for i in ru_text:\n",
        "  ru_str += i + ' '\n",
        "ru_str = ru_str.replace('  ', ' ')"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfWMADJqfpT9"
      },
      "source": [
        "# Для второй части"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTpxZZuAN-yV"
      },
      "source": [
        "def make_word_list(text):\n",
        "  word_list = clean(text).replace('\\n', ' ').split()\n",
        "  return word_list"
      ],
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3xzdxDLJULQ"
      },
      "source": [
        "def make_pos(word_list):\n",
        "  tags_list = []\n",
        "  for w in word_list:\n",
        "    doc = Doc(w)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    tags_list.append((nts_normalized(doc.tokens[0].pos)))\n",
        "  return tags_list"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qL5cgjMhKUgp"
      },
      "source": [
        "def find_bigrams(words_list, tags_list):\n",
        "  not_verb = []\n",
        "  adj_noun = []\n",
        "  adv_adj = []\n",
        "  for i in range(len(words_list)):\n",
        "    try:\n",
        "      if words_list[i] == 'не' and tags_list[i+1] == 'VERB':\n",
        "        not_verb.append(words_list[i] + ' ' + words_list[i+1])\n",
        "      if tags_list[i] == 'ADJ' and tags_list[i+1] == 'NOUN':\n",
        "        adj_noun.append(words_list[i] + ' ' + words_list[i+1])\n",
        "      if tags_list[i] == 'ADV' and tags_list[i+1] == 'ADJ':\n",
        "        adv_adj.append(words_list[i] + ' ' + words_list[i+1])\n",
        "    except IndexError:\n",
        "      pass\n",
        "  return not_verb, adj_noun, adv_adj"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymRwQbAUQarr"
      },
      "source": [
        ""
      ],
      "execution_count": 240,
      "outputs": []
    }
  ]
}